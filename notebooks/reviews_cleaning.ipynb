{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a chunk of data\n",
    "def process_data(chunk, columns=None):\n",
    "    # If columns is not None, keep only those columns\n",
    "    if columns is not None:\n",
    "        chunk = chunk[columns]\n",
    "    return chunk\n",
    "\n",
    "# Function to read data in chunks and process each chunk\n",
    "def load_data(file_name, head = None, columns=None, chunksize = 1000):\n",
    "    chunks = []\n",
    "    count = 0\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for chunk in pd.read_json(fin, lines=True, chunksize=chunksize):\n",
    "            # Process the chunk\n",
    "            processed_chunk = process_data(chunk, columns)\n",
    "            chunks.append(processed_chunk)\n",
    "            \n",
    "            count += 1\n",
    "            # break if reaches the head-th chunk\n",
    "            if (head is not None) and (count > head):\n",
    "                break\n",
    "\n",
    "    # Combine all chunks into a single DataFrame\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'C:\\\\Users\\\\jesse\\\\Desktop\\\\Honors Project\\\\goodreads_data\\\\raw\\\\'\n",
    "ya_books = load_data(DIR + 'goodreads_books_young_adult.json.gz', head = 1)\n",
    "ya_reviews = load_data(DIR + 'goodreads_reviews_young_adult.json.gz', head = 1)\n",
    "ya_interactions = load_data(DIR + 'goodreads_interactions_young_adult.json.gz', head = 1)\n",
    "#books_works = load_data(DIR + 'goodreads_book_works.json.gz', head = 1)\n",
    "#reviews = load_data(DIR + 'goodreads_reviews_spoiler.json.gz', head = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>2767052</td>\n",
       "      <td>248c011811e945eca861b5c31a549291</td>\n",
       "      <td>5</td>\n",
       "      <td>I cracked and finally picked this up. Very enj...</td>\n",
       "      <td>Wed Jan 13 13:38:25 -0800 2010</td>\n",
       "      <td>Wed Mar 22 11:46:36 -0700 2017</td>\n",
       "      <td>Sun Mar 25 00:00:00 -0700 2012</td>\n",
       "      <td>Fri Mar 23 00:00:00 -0700 2012</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7504b2aee1ecb5b2872d3da381c6c91e</td>\n",
       "      <td>23302416</td>\n",
       "      <td>84c0936a0f9868f38e75d2f9a5cb761e</td>\n",
       "      <td>5</td>\n",
       "      <td>I read this book because my fifth grade son wa...</td>\n",
       "      <td>Wed Jan 21 18:40:59 -0800 2015</td>\n",
       "      <td>Wed Oct 26 03:44:13 -0700 2016</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f8a89075dc6de14857561522e729f82c</td>\n",
       "      <td>18053080</td>\n",
       "      <td>785c8db878f4009da9741dea51f641da</td>\n",
       "      <td>4</td>\n",
       "      <td>Though the book started out slow and only star...</td>\n",
       "      <td>Sat Jan 11 17:58:41 -0800 2014</td>\n",
       "      <td>Tue Dec 02 11:43:07 -0800 2014</td>\n",
       "      <td>Sat Apr 12 00:00:00 -0700 2014</td>\n",
       "      <td>Fri Apr 11 00:00:00 -0700 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8a89075dc6de14857561522e729f82c</td>\n",
       "      <td>17383543</td>\n",
       "      <td>34dc3c45d07e82718b05e73167259aef</td>\n",
       "      <td>2</td>\n",
       "      <td>*Update - 10/27/13* - After some sleep, I thin...</td>\n",
       "      <td>Sun Apr 21 19:42:28 -0700 2013</td>\n",
       "      <td>Fri Aug 15 07:55:01 -0700 2014</td>\n",
       "      <td>Sat Oct 26 00:00:00 -0700 2013</td>\n",
       "      <td>Fri Oct 25 00:00:00 -0700 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f8a89075dc6de14857561522e729f82c</td>\n",
       "      <td>16651458</td>\n",
       "      <td>d8d6b590780256fef7ae4a9550fe3e0d</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a moving, heartbreaking, view into a l...</td>\n",
       "      <td>Fri Jan 11 11:42:42 -0800 2013</td>\n",
       "      <td>Fri Mar 01 09:31:01 -0800 2013</td>\n",
       "      <td>Mon Jan 14 00:00:00 -0800 2013</td>\n",
       "      <td>Sat Jan 12 00:00:00 -0800 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>11235712</td>\n",
       "      <td>3e0f18f959ee2a25f82afc04c56e9111</td>\n",
       "      <td>4</td>\n",
       "      <td>I really like this twist on the classic Cinder...</td>\n",
       "      <td>Sun Jun 17 11:48:14 -0700 2012</td>\n",
       "      <td>Mon Oct 20 08:16:14 -0700 2014</td>\n",
       "      <td>Sun Oct 19 00:00:00 -0700 2014</td>\n",
       "      <td>Mon Oct 13 00:00:00 -0700 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>12810834</td>\n",
       "      <td>008569bed43fcee3313c574266fc0b05</td>\n",
       "      <td>3</td>\n",
       "      <td>Interesting and well written. Starts a little ...</td>\n",
       "      <td>Thu Apr 26 16:43:55 -0700 2012</td>\n",
       "      <td>Fri Aug 25 11:25:03 -0700 2017</td>\n",
       "      <td>Tue May 01 00:00:00 -0700 2012</td>\n",
       "      <td>Thu Apr 26 00:00:00 -0700 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>27183386</td>\n",
       "      <td>31f06dc1af01a471637d83123150a32f</td>\n",
       "      <td>3</td>\n",
       "      <td>It took me 2 tries to finish this book, but I ...</td>\n",
       "      <td>Fri Mar 30 19:45:28 -0700 2012</td>\n",
       "      <td>Wed Apr 26 17:43:12 -0700 2017</td>\n",
       "      <td>Wed Apr 26 00:00:00 -0700 2017</td>\n",
       "      <td>Sat Apr 22 00:00:00 -0700 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>8306857</td>\n",
       "      <td>a7aded22d01dcadf7b75ed595f1764c9</td>\n",
       "      <td>5</td>\n",
       "      <td>Wow! If you're looking for the next Hunger Gam...</td>\n",
       "      <td>Fri Mar 30 19:41:19 -0700 2012</td>\n",
       "      <td>Sat May 28 09:47:32 -0700 2016</td>\n",
       "      <td>Sun Apr 01 10:07:50 -0700 2012</td>\n",
       "      <td>Fri Mar 30 00:00:00 -0700 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>9462883</td>\n",
       "      <td>adf545f2c9afcbf430795dad40139f02</td>\n",
       "      <td>3</td>\n",
       "      <td>I think the audiobook may have tainted my opin...</td>\n",
       "      <td>Mon Apr 25 11:12:52 -0700 2011</td>\n",
       "      <td>Sun Jul 09 19:41:39 -0700 2017</td>\n",
       "      <td>Sun Jul 09 00:00:00 -0700 2017</td>\n",
       "      <td>Fri Jun 30 00:00:00 -0700 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               user_id   book_id  \\\n",
       "0     8842281e1d1347389f2ab93d60773d4d   2767052   \n",
       "1     7504b2aee1ecb5b2872d3da381c6c91e  23302416   \n",
       "2     f8a89075dc6de14857561522e729f82c  18053080   \n",
       "3     f8a89075dc6de14857561522e729f82c  17383543   \n",
       "4     f8a89075dc6de14857561522e729f82c  16651458   \n",
       "...                                ...       ...   \n",
       "1995  d1e368a7d2870eb6fbf6e0d350568a2d  11235712   \n",
       "1996  d1e368a7d2870eb6fbf6e0d350568a2d  12810834   \n",
       "1997  d1e368a7d2870eb6fbf6e0d350568a2d  27183386   \n",
       "1998  d1e368a7d2870eb6fbf6e0d350568a2d   8306857   \n",
       "1999  d1e368a7d2870eb6fbf6e0d350568a2d   9462883   \n",
       "\n",
       "                             review_id  rating  \\\n",
       "0     248c011811e945eca861b5c31a549291       5   \n",
       "1     84c0936a0f9868f38e75d2f9a5cb761e       5   \n",
       "2     785c8db878f4009da9741dea51f641da       4   \n",
       "3     34dc3c45d07e82718b05e73167259aef       2   \n",
       "4     d8d6b590780256fef7ae4a9550fe3e0d       5   \n",
       "...                                ...     ...   \n",
       "1995  3e0f18f959ee2a25f82afc04c56e9111       4   \n",
       "1996  008569bed43fcee3313c574266fc0b05       3   \n",
       "1997  31f06dc1af01a471637d83123150a32f       3   \n",
       "1998  a7aded22d01dcadf7b75ed595f1764c9       5   \n",
       "1999  adf545f2c9afcbf430795dad40139f02       3   \n",
       "\n",
       "                                            review_text  \\\n",
       "0     I cracked and finally picked this up. Very enj...   \n",
       "1     I read this book because my fifth grade son wa...   \n",
       "2     Though the book started out slow and only star...   \n",
       "3     *Update - 10/27/13* - After some sleep, I thin...   \n",
       "4     This is a moving, heartbreaking, view into a l...   \n",
       "...                                                 ...   \n",
       "1995  I really like this twist on the classic Cinder...   \n",
       "1996  Interesting and well written. Starts a little ...   \n",
       "1997  It took me 2 tries to finish this book, but I ...   \n",
       "1998  Wow! If you're looking for the next Hunger Gam...   \n",
       "1999  I think the audiobook may have tainted my opin...   \n",
       "\n",
       "                          date_added                    date_updated  \\\n",
       "0     Wed Jan 13 13:38:25 -0800 2010  Wed Mar 22 11:46:36 -0700 2017   \n",
       "1     Wed Jan 21 18:40:59 -0800 2015  Wed Oct 26 03:44:13 -0700 2016   \n",
       "2     Sat Jan 11 17:58:41 -0800 2014  Tue Dec 02 11:43:07 -0800 2014   \n",
       "3     Sun Apr 21 19:42:28 -0700 2013  Fri Aug 15 07:55:01 -0700 2014   \n",
       "4     Fri Jan 11 11:42:42 -0800 2013  Fri Mar 01 09:31:01 -0800 2013   \n",
       "...                              ...                             ...   \n",
       "1995  Sun Jun 17 11:48:14 -0700 2012  Mon Oct 20 08:16:14 -0700 2014   \n",
       "1996  Thu Apr 26 16:43:55 -0700 2012  Fri Aug 25 11:25:03 -0700 2017   \n",
       "1997  Fri Mar 30 19:45:28 -0700 2012  Wed Apr 26 17:43:12 -0700 2017   \n",
       "1998  Fri Mar 30 19:41:19 -0700 2012  Sat May 28 09:47:32 -0700 2016   \n",
       "1999  Mon Apr 25 11:12:52 -0700 2011  Sun Jul 09 19:41:39 -0700 2017   \n",
       "\n",
       "                             read_at                      started_at  n_votes  \\\n",
       "0     Sun Mar 25 00:00:00 -0700 2012  Fri Mar 23 00:00:00 -0700 2012       24   \n",
       "1                                                                           0   \n",
       "2     Sat Apr 12 00:00:00 -0700 2014  Fri Apr 11 00:00:00 -0700 2014        0   \n",
       "3     Sat Oct 26 00:00:00 -0700 2013  Fri Oct 25 00:00:00 -0700 2013        0   \n",
       "4     Mon Jan 14 00:00:00 -0800 2013  Sat Jan 12 00:00:00 -0800 2013        0   \n",
       "...                              ...                             ...      ...   \n",
       "1995  Sun Oct 19 00:00:00 -0700 2014  Mon Oct 13 00:00:00 -0700 2014        0   \n",
       "1996  Tue May 01 00:00:00 -0700 2012  Thu Apr 26 00:00:00 -0700 2012        0   \n",
       "1997  Wed Apr 26 00:00:00 -0700 2017  Sat Apr 22 00:00:00 -0700 2017        0   \n",
       "1998  Sun Apr 01 10:07:50 -0700 2012  Fri Mar 30 00:00:00 -0700 2012        0   \n",
       "1999  Sun Jul 09 00:00:00 -0700 2017  Fri Jun 30 00:00:00 -0700 2017        0   \n",
       "\n",
       "      n_comments  \n",
       "0             25  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "1995           0  \n",
       "1996           0  \n",
       "1997           0  \n",
       "1998           0  \n",
       "1999           0  \n",
       "\n",
       "[2000 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep from ya_reviews:\n",
    "['book_id', 'review_id', 'rating', 'review_text', 'n_votes', 'n_comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2389900 entries, 0 to 2389899\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   book_id      int64 \n",
      " 1   review_id    object\n",
      " 2   rating       int64 \n",
      " 3   review_text  object\n",
      " 4   n_votes      int64 \n",
      " 5   n_comments   int64 \n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 109.4+ MB\n"
     ]
    }
   ],
   "source": [
    "ya_reviews = load_data(DIR + 'goodreads_reviews_young_adult.json.gz', columns = ['book_id', 'review_id', 'rating', 'review_text', 'n_votes', 'n_comments'])\n",
    "ya_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 36524503 book_id\n",
      "0 5 rating\n",
      "-3 3942 n_votes\n",
      "-3 922 n_comments\n"
     ]
    }
   ],
   "source": [
    "#print out the min and max for ya_reviews['book_id'] and rating and n_votes and n_comments\n",
    "#for int and float types, print out the min and max, followed by the column name\n",
    "for col in ya_reviews.columns:\n",
    "    if ya_reviews[col].dtype == 'int64' or ya_reviews[col].dtype == 'float64':\n",
    "        print(ya_reviews[col].min(), ya_reviews[col].max(), col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory reasons, downgrade to min int fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ya_reviews['book_id'] = ya_reviews['book_id'].astype('int32')\n",
    "ya_reviews['rating'] = ya_reviews['rating'].astype('int8')\n",
    "ya_reviews['n_votes'] = ya_reviews['n_votes'].astype('int16')\n",
    "ya_reviews['n_comments'] = ya_reviews['n_comments'].astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicate review_id in ya_reviews\n",
    "ya_reviews['review_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace blank values with NaN\n",
    "ya_books.replace('', np.nan, inplace=True)\n",
    "ya_reviews.replace('', np.nan, inplace=True)\n",
    "ya_interactions.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicate review_id in ya_reviews\n",
    "ya_reviews['review_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2389900 entries, 0 to 2389899\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   book_id      int32 \n",
      " 1   review_id    object\n",
      " 2   rating       int8  \n",
      " 3   review_text  object\n",
      " 4   n_votes      int16 \n",
      " 5   n_comments   int16 \n",
      "dtypes: int16(2), int32(1), int8(1), object(2)\n",
      "memory usage: 57.0+ MB\n"
     ]
    }
   ],
   "source": [
    "ya_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no nulls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
