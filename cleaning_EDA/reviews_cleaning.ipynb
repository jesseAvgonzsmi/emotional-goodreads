{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing and prelim column selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process a chunk of data\n",
    "def process_data(chunk, columns=None):\n",
    "    # If columns is not None, keep only those columns\n",
    "    if columns is not None:\n",
    "        chunk = chunk[columns]\n",
    "    return chunk\n",
    "\n",
    "# Function to read data in chunks and process each chunk\n",
    "def load_data(file_name, head = None, columns=None, chunksize = 1000):\n",
    "    chunks = []\n",
    "    count = 0\n",
    "    with gzip.open(file_name) as fin:\n",
    "        for chunk in pd.read_json(fin, lines=True, chunksize=chunksize):\n",
    "            # Process the chunk\n",
    "            processed_chunk = process_data(chunk, columns)\n",
    "            chunks.append(processed_chunk)\n",
    "            \n",
    "            count += 1\n",
    "            # break if reaches the head-th chunk\n",
    "            if (head is not None) and (count > head):\n",
    "                break\n",
    "\n",
    "    # Combine all chunks into a single DataFrame\n",
    "    df = pd.concat(chunks, ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'C:\\\\Users\\\\jesse\\\\Desktop\\\\Honors Project\\\\goodreads_data\\\\raw\\\\'\n",
    "ya_reviews = load_data(DIR + 'goodreads_reviews_young_adult.json.gz', head = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>date_added</th>\n",
       "      <th>date_updated</th>\n",
       "      <th>read_at</th>\n",
       "      <th>started_at</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8842281e1d1347389f2ab93d60773d4d</td>\n",
       "      <td>2767052</td>\n",
       "      <td>248c011811e945eca861b5c31a549291</td>\n",
       "      <td>5</td>\n",
       "      <td>I cracked and finally picked this up. Very enj...</td>\n",
       "      <td>Wed Jan 13 13:38:25 -0800 2010</td>\n",
       "      <td>Wed Mar 22 11:46:36 -0700 2017</td>\n",
       "      <td>Sun Mar 25 00:00:00 -0700 2012</td>\n",
       "      <td>Fri Mar 23 00:00:00 -0700 2012</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7504b2aee1ecb5b2872d3da381c6c91e</td>\n",
       "      <td>23302416</td>\n",
       "      <td>84c0936a0f9868f38e75d2f9a5cb761e</td>\n",
       "      <td>5</td>\n",
       "      <td>I read this book because my fifth grade son wa...</td>\n",
       "      <td>Wed Jan 21 18:40:59 -0800 2015</td>\n",
       "      <td>Wed Oct 26 03:44:13 -0700 2016</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f8a89075dc6de14857561522e729f82c</td>\n",
       "      <td>18053080</td>\n",
       "      <td>785c8db878f4009da9741dea51f641da</td>\n",
       "      <td>4</td>\n",
       "      <td>Though the book started out slow and only star...</td>\n",
       "      <td>Sat Jan 11 17:58:41 -0800 2014</td>\n",
       "      <td>Tue Dec 02 11:43:07 -0800 2014</td>\n",
       "      <td>Sat Apr 12 00:00:00 -0700 2014</td>\n",
       "      <td>Fri Apr 11 00:00:00 -0700 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f8a89075dc6de14857561522e729f82c</td>\n",
       "      <td>17383543</td>\n",
       "      <td>34dc3c45d07e82718b05e73167259aef</td>\n",
       "      <td>2</td>\n",
       "      <td>*Update - 10/27/13* - After some sleep, I thin...</td>\n",
       "      <td>Sun Apr 21 19:42:28 -0700 2013</td>\n",
       "      <td>Fri Aug 15 07:55:01 -0700 2014</td>\n",
       "      <td>Sat Oct 26 00:00:00 -0700 2013</td>\n",
       "      <td>Fri Oct 25 00:00:00 -0700 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f8a89075dc6de14857561522e729f82c</td>\n",
       "      <td>16651458</td>\n",
       "      <td>d8d6b590780256fef7ae4a9550fe3e0d</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a moving, heartbreaking, view into a l...</td>\n",
       "      <td>Fri Jan 11 11:42:42 -0800 2013</td>\n",
       "      <td>Fri Mar 01 09:31:01 -0800 2013</td>\n",
       "      <td>Mon Jan 14 00:00:00 -0800 2013</td>\n",
       "      <td>Sat Jan 12 00:00:00 -0800 2013</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>11235712</td>\n",
       "      <td>3e0f18f959ee2a25f82afc04c56e9111</td>\n",
       "      <td>4</td>\n",
       "      <td>I really like this twist on the classic Cinder...</td>\n",
       "      <td>Sun Jun 17 11:48:14 -0700 2012</td>\n",
       "      <td>Mon Oct 20 08:16:14 -0700 2014</td>\n",
       "      <td>Sun Oct 19 00:00:00 -0700 2014</td>\n",
       "      <td>Mon Oct 13 00:00:00 -0700 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>12810834</td>\n",
       "      <td>008569bed43fcee3313c574266fc0b05</td>\n",
       "      <td>3</td>\n",
       "      <td>Interesting and well written. Starts a little ...</td>\n",
       "      <td>Thu Apr 26 16:43:55 -0700 2012</td>\n",
       "      <td>Fri Aug 25 11:25:03 -0700 2017</td>\n",
       "      <td>Tue May 01 00:00:00 -0700 2012</td>\n",
       "      <td>Thu Apr 26 00:00:00 -0700 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>27183386</td>\n",
       "      <td>31f06dc1af01a471637d83123150a32f</td>\n",
       "      <td>3</td>\n",
       "      <td>It took me 2 tries to finish this book, but I ...</td>\n",
       "      <td>Fri Mar 30 19:45:28 -0700 2012</td>\n",
       "      <td>Wed Apr 26 17:43:12 -0700 2017</td>\n",
       "      <td>Wed Apr 26 00:00:00 -0700 2017</td>\n",
       "      <td>Sat Apr 22 00:00:00 -0700 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>8306857</td>\n",
       "      <td>a7aded22d01dcadf7b75ed595f1764c9</td>\n",
       "      <td>5</td>\n",
       "      <td>Wow! If you're looking for the next Hunger Gam...</td>\n",
       "      <td>Fri Mar 30 19:41:19 -0700 2012</td>\n",
       "      <td>Sat May 28 09:47:32 -0700 2016</td>\n",
       "      <td>Sun Apr 01 10:07:50 -0700 2012</td>\n",
       "      <td>Fri Mar 30 00:00:00 -0700 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>d1e368a7d2870eb6fbf6e0d350568a2d</td>\n",
       "      <td>9462883</td>\n",
       "      <td>adf545f2c9afcbf430795dad40139f02</td>\n",
       "      <td>3</td>\n",
       "      <td>I think the audiobook may have tainted my opin...</td>\n",
       "      <td>Mon Apr 25 11:12:52 -0700 2011</td>\n",
       "      <td>Sun Jul 09 19:41:39 -0700 2017</td>\n",
       "      <td>Sun Jul 09 00:00:00 -0700 2017</td>\n",
       "      <td>Fri Jun 30 00:00:00 -0700 2017</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               user_id   book_id  \\\n",
       "0     8842281e1d1347389f2ab93d60773d4d   2767052   \n",
       "1     7504b2aee1ecb5b2872d3da381c6c91e  23302416   \n",
       "2     f8a89075dc6de14857561522e729f82c  18053080   \n",
       "3     f8a89075dc6de14857561522e729f82c  17383543   \n",
       "4     f8a89075dc6de14857561522e729f82c  16651458   \n",
       "...                                ...       ...   \n",
       "1995  d1e368a7d2870eb6fbf6e0d350568a2d  11235712   \n",
       "1996  d1e368a7d2870eb6fbf6e0d350568a2d  12810834   \n",
       "1997  d1e368a7d2870eb6fbf6e0d350568a2d  27183386   \n",
       "1998  d1e368a7d2870eb6fbf6e0d350568a2d   8306857   \n",
       "1999  d1e368a7d2870eb6fbf6e0d350568a2d   9462883   \n",
       "\n",
       "                             review_id  rating  \\\n",
       "0     248c011811e945eca861b5c31a549291       5   \n",
       "1     84c0936a0f9868f38e75d2f9a5cb761e       5   \n",
       "2     785c8db878f4009da9741dea51f641da       4   \n",
       "3     34dc3c45d07e82718b05e73167259aef       2   \n",
       "4     d8d6b590780256fef7ae4a9550fe3e0d       5   \n",
       "...                                ...     ...   \n",
       "1995  3e0f18f959ee2a25f82afc04c56e9111       4   \n",
       "1996  008569bed43fcee3313c574266fc0b05       3   \n",
       "1997  31f06dc1af01a471637d83123150a32f       3   \n",
       "1998  a7aded22d01dcadf7b75ed595f1764c9       5   \n",
       "1999  adf545f2c9afcbf430795dad40139f02       3   \n",
       "\n",
       "                                            review_text  \\\n",
       "0     I cracked and finally picked this up. Very enj...   \n",
       "1     I read this book because my fifth grade son wa...   \n",
       "2     Though the book started out slow and only star...   \n",
       "3     *Update - 10/27/13* - After some sleep, I thin...   \n",
       "4     This is a moving, heartbreaking, view into a l...   \n",
       "...                                                 ...   \n",
       "1995  I really like this twist on the classic Cinder...   \n",
       "1996  Interesting and well written. Starts a little ...   \n",
       "1997  It took me 2 tries to finish this book, but I ...   \n",
       "1998  Wow! If you're looking for the next Hunger Gam...   \n",
       "1999  I think the audiobook may have tainted my opin...   \n",
       "\n",
       "                          date_added                    date_updated  \\\n",
       "0     Wed Jan 13 13:38:25 -0800 2010  Wed Mar 22 11:46:36 -0700 2017   \n",
       "1     Wed Jan 21 18:40:59 -0800 2015  Wed Oct 26 03:44:13 -0700 2016   \n",
       "2     Sat Jan 11 17:58:41 -0800 2014  Tue Dec 02 11:43:07 -0800 2014   \n",
       "3     Sun Apr 21 19:42:28 -0700 2013  Fri Aug 15 07:55:01 -0700 2014   \n",
       "4     Fri Jan 11 11:42:42 -0800 2013  Fri Mar 01 09:31:01 -0800 2013   \n",
       "...                              ...                             ...   \n",
       "1995  Sun Jun 17 11:48:14 -0700 2012  Mon Oct 20 08:16:14 -0700 2014   \n",
       "1996  Thu Apr 26 16:43:55 -0700 2012  Fri Aug 25 11:25:03 -0700 2017   \n",
       "1997  Fri Mar 30 19:45:28 -0700 2012  Wed Apr 26 17:43:12 -0700 2017   \n",
       "1998  Fri Mar 30 19:41:19 -0700 2012  Sat May 28 09:47:32 -0700 2016   \n",
       "1999  Mon Apr 25 11:12:52 -0700 2011  Sun Jul 09 19:41:39 -0700 2017   \n",
       "\n",
       "                             read_at                      started_at  n_votes  \\\n",
       "0     Sun Mar 25 00:00:00 -0700 2012  Fri Mar 23 00:00:00 -0700 2012       24   \n",
       "1                                                                           0   \n",
       "2     Sat Apr 12 00:00:00 -0700 2014  Fri Apr 11 00:00:00 -0700 2014        0   \n",
       "3     Sat Oct 26 00:00:00 -0700 2013  Fri Oct 25 00:00:00 -0700 2013        0   \n",
       "4     Mon Jan 14 00:00:00 -0800 2013  Sat Jan 12 00:00:00 -0800 2013        0   \n",
       "...                              ...                             ...      ...   \n",
       "1995  Sun Oct 19 00:00:00 -0700 2014  Mon Oct 13 00:00:00 -0700 2014        0   \n",
       "1996  Tue May 01 00:00:00 -0700 2012  Thu Apr 26 00:00:00 -0700 2012        0   \n",
       "1997  Wed Apr 26 00:00:00 -0700 2017  Sat Apr 22 00:00:00 -0700 2017        0   \n",
       "1998  Sun Apr 01 10:07:50 -0700 2012  Fri Mar 30 00:00:00 -0700 2012        0   \n",
       "1999  Sun Jul 09 00:00:00 -0700 2017  Fri Jun 30 00:00:00 -0700 2017        0   \n",
       "\n",
       "      n_comments  \n",
       "0             25  \n",
       "1              0  \n",
       "2              0  \n",
       "3              0  \n",
       "4              0  \n",
       "...          ...  \n",
       "1995           0  \n",
       "1996           0  \n",
       "1997           0  \n",
       "1998           0  \n",
       "1999           0  \n",
       "\n",
       "[2000 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep from ya_reviews:\n",
    "['book_id', 'review_id', 'rating', 'review_text', 'n_votes', 'n_comments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m ya_reviews \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDIR\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgoodreads_reviews_young_adult.json.gz\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbook_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreview_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_votes\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_comments\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m ya_reviews\u001b[38;5;241m.\u001b[39minfo()\n",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(file_name, head, columns, chunksize)\u001b[0m\n\u001b[0;32m     11\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m gzip\u001b[38;5;241m.\u001b[39mopen(file_name) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_json(fin, lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, chunksize\u001b[38;5;241m=\u001b[39mchunksize):\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;66;03m# Process the chunk\u001b[39;00m\n\u001b[0;32m     15\u001b[0m         processed_chunk \u001b[38;5;241m=\u001b[39m process_data(chunk, columns)\n\u001b[0;32m     16\u001b[0m         chunks\u001b[38;5;241m.\u001b[39mappend(processed_chunk)\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1097\u001b[0m, in \u001b[0;36mJsonReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1096\u001b[0m     lines_json \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_lines(lines)\n\u001b[1;32m-> 1097\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object_parser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlines_json\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;66;03m# Make sure that the returned objects have the right index.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m     obj\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows_seen, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows_seen \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(obj))\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1051\u001b[0m, in \u001b[0;36mJsonReader._get_object_parser\u001b[1;34m(self, json)\u001b[0m\n\u001b[0;32m   1049\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1050\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1051\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43mFrameParser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseries\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, \u001b[38;5;28mbool\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1193\u001b[0m, in \u001b[0;36mParser.parse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_axes:\n\u001b[0;32m   1192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_axes()\n\u001b[1;32m-> 1193\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_convert_types\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1464\u001b[0m, in \u001b[0;36mFrameParser._try_convert_types\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1462\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dates:\n\u001b[1;32m-> 1464\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_convert_dates\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_converter(\n\u001b[0;32m   1467\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m col, c: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_convert_data(col, c, convert_dates\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1468\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1502\u001b[0m, in \u001b[0;36mFrameParser._try_convert_dates\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1502\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_converter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_convert_to_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_ok\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1448\u001b[0m, in \u001b[0;36mFrameParser._process_converter\u001b[1;34m(self, f, filt)\u001b[0m\n\u001b[0;32m   1446\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (col, c) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(obj\u001b[38;5;241m.\u001b[39mitems()):\n\u001b[0;32m   1447\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filt(col):\n\u001b[1;32m-> 1448\u001b[0m         new_data, result \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1449\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m   1450\u001b[0m             c \u001b[38;5;241m=\u001b[39m new_data\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1502\u001b[0m, in \u001b[0;36mFrameParser._try_convert_dates.<locals>.<lambda>\u001b[1;34m(col, c)\u001b[0m\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 1502\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_converter(\u001b[38;5;28;01mlambda\u001b[39;00m col, c: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_convert_to_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m)\u001b[49m, filt\u001b[38;5;241m=\u001b[39mis_ok)\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\io\\json\\_json.py:1357\u001b[0m, in \u001b[0;36mParser._try_convert_to_date\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n\u001b[0;32m   1351\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\n\u001b[0;32m   1352\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1353\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.*parsing datetimes with mixed time \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1354\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzones will raise an error\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1355\u001b[0m             category\u001b[38;5;241m=\u001b[39m\u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m   1356\u001b[0m         )\n\u001b[1;32m-> 1357\u001b[0m         new_data \u001b[38;5;241m=\u001b[39m \u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraise\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdate_unit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1358\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mOverflowError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m   1359\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:1063\u001b[0m, in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m   1061\u001b[0m             result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mtz_localize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, ABCSeries):\n\u001b[1;32m-> 1063\u001b[0m     cache_array \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_listlike\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m cache_array\u001b[38;5;241m.\u001b[39mempty:\n\u001b[0;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:247\u001b[0m, in \u001b[0;36m_maybe_cache\u001b[1;34m(arg, format, cache, convert_listlike)\u001b[0m\n\u001b[0;32m    245\u001b[0m unique_dates \u001b[38;5;241m=\u001b[39m unique(arg)\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_dates) \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(arg):\n\u001b[1;32m--> 247\u001b[0m     cache_dates \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_dates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# GH#45319\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:407\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[1;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot specify both format and unit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 407\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_to_datetime_with_unit\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg must be a string, datetime, list, tuple, 1-d array, or Series\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    411\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jesse\\anaconda3\\Lib\\site-packages\\pandas\\core\\tools\\datetimes.py:526\u001b[0m, in \u001b[0;36m_to_datetime_with_unit\u001b[1;34m(arg, unit, name, utc, errors)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    525\u001b[0m         arg \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 526\u001b[0m         arr, tz_parsed \u001b[38;5;241m=\u001b[39m \u001b[43mtslib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_with_unit_to_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    529\u001b[0m     \u001b[38;5;66;03m# Index constructor _may_ infer to DatetimeIndex\u001b[39;00m\n\u001b[0;32m    530\u001b[0m     result \u001b[38;5;241m=\u001b[39m Index\u001b[38;5;241m.\u001b[39m_with_infer(arr, name\u001b[38;5;241m=\u001b[39mname)\n",
      "File \u001b[1;32mtslib.pyx:285\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_with_unit_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtslib.pyx:414\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mtslib.pyx:553\u001b[0m, in \u001b[0;36mpandas._libs.tslib.array_to_datetime\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mconversion.pyx:641\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.conversion.convert_str_to_tsobject\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:336\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.parse_datetime_string\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsing.pyx:660\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.parsing.dateutil_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\dateutil\\parser\\_parser.py:724\u001b[0m, in \u001b[0;36mparser._parse\u001b[1;34m(self, timestr, dayfirst, yearfirst, fuzzy, fuzzy_with_tokens)\u001b[0m\n\u001b[0;32m    721\u001b[0m skipped_idxs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    723\u001b[0m \u001b[38;5;66;03m# year/month/day list\u001b[39;00m\n\u001b[1;32m--> 724\u001b[0m ymd \u001b[38;5;241m=\u001b[39m \u001b[43m_ymd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m len_l \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(l)\n\u001b[0;32m    727\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\dateutil\\parser\\_parser.py:395\u001b[0m, in \u001b[0;36m_ymd.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ymd\u001b[39;00m(\u001b[38;5;28mlist\u001b[39m):\n\u001b[1;32m--> 395\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    396\u001b[0m         \u001b[38;5;28msuper\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcentury_specified \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ya_reviews = load_data(DIR + 'goodreads_reviews_young_adult.json.gz', columns = ['book_id','review_id', 'rating', 'review_text', 'n_votes', 'n_comments'])\n",
    "ya_reviews.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for duplicate review_id in ya_reviews\n",
    "ya_reviews['review_id'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 36524503 book_id\n",
      "0 5 rating\n",
      "-3 3942 n_votes\n",
      "-3 922 n_comments\n"
     ]
    }
   ],
   "source": [
    "#print out the min and max for ya_reviews['book_id'] and rating and n_votes and n_comments\n",
    "#for int and float types, print out the min and max, followed by the column name\n",
    "for col in ya_reviews.columns:\n",
    "    if ya_reviews[col].dtype == 'int64' or ya_reviews[col].dtype == 'float64':\n",
    "        print(ya_reviews[col].min(), ya_reviews[col].max(), col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory reasons, downgrade to min int fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ya_reviews['book_id'] = ya_reviews['book_id'].astype('int32')\n",
    "ya_reviews['rating'] = ya_reviews['rating'].astype('int8')\n",
    "ya_reviews['n_votes'] = ya_reviews['n_votes'].astype('int16')\n",
    "ya_reviews['n_comments'] = ya_reviews['n_comments'].astype('int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace blank values with NaN\n",
    "ya_reviews.replace('', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2389900 entries, 0 to 2389899\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Dtype \n",
      "---  ------       ----- \n",
      " 0   book_id      int32 \n",
      " 1   review_id    object\n",
      " 2   rating       int8  \n",
      " 3   review_text  object\n",
      " 4   n_votes      int16 \n",
      " 5   n_comments   int16 \n",
      "dtypes: int16(2), int32(1), int8(1), object(2)\n",
      "memory usage: 57.0+ MB\n"
     ]
    }
   ],
   "source": [
    "ya_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "no nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "book_id           0\n",
       "review_id         0\n",
       "rating            0\n",
       "review_text    1246\n",
       "n_votes           0\n",
       "n_comments        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check for nulls in ya_reviews\n",
    "ya_reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop rows with nulls\n",
    "ya_reviews.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing review text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>review_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>review_text</th>\n",
       "      <th>n_votes</th>\n",
       "      <th>n_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2767052</td>\n",
       "      <td>248c011811e945eca861b5c31a549291</td>\n",
       "      <td>5</td>\n",
       "      <td>I cracked and finally picked this up. Very enjoyable quick read - couldn't put it down - it was like crack. \\n I'm a bit bothered by the lack of backstory of how Panem and the Hunger Games come ab...</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>23302416</td>\n",
       "      <td>84c0936a0f9868f38e75d2f9a5cb761e</td>\n",
       "      <td>5</td>\n",
       "      <td>I read this book because my fifth grade son was required to for school. I'm so glad I did! I experienced a range of emotions &amp; just loved it. Glad these middle schoolers are being exposed to the t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18053080</td>\n",
       "      <td>785c8db878f4009da9741dea51f641da</td>\n",
       "      <td>4</td>\n",
       "      <td>Though the book started out slow and only started to get interesting towards page 100, overall, it was worth the read.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17383543</td>\n",
       "      <td>34dc3c45d07e82718b05e73167259aef</td>\n",
       "      <td>2</td>\n",
       "      <td>*Update - 10/27/13* - After some sleep, I thinking about Allegiant overall without the influence of other reviews, I changed my rating to a 2 which is deserved. \\n I know that no author is going t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16651458</td>\n",
       "      <td>d8d6b590780256fef7ae4a9550fe3e0d</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a moving, heartbreaking, view into a life of an obese 16 year old boy in high school. Don't let the name \"butter\" cause you to overlook reading this book. You see the life of butter from h...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389895</th>\n",
       "      <td>6137154</td>\n",
       "      <td>1f6fc20b8187e8f36e12daf947b13e66</td>\n",
       "      <td>5</td>\n",
       "      <td>A really amazing book! It goes great with Graceling. It's connected to it in a way you don't realize until the end of either books. It really is amazing and semi-mind twisting.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389896</th>\n",
       "      <td>10193062</td>\n",
       "      <td>fc86ebe129a7f14a09c0221c92f52f79</td>\n",
       "      <td>3</td>\n",
       "      <td>This book is better then the first. It has it's slow points \\n (mostly the romance sections. I don't know why but it just doesn't interest me as much as it did in other books. Might be because I l...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389897</th>\n",
       "      <td>6186357</td>\n",
       "      <td>15d0f64aa991f270c490e1a4d5b4011b</td>\n",
       "      <td>2</td>\n",
       "      <td>I think this may have come from having expectations set to high. Everything I heard and read I was expecting to be blown away and I simply wasn't. I really don't like Thomas, I think he's an Idiot...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389898</th>\n",
       "      <td>14740456</td>\n",
       "      <td>b048505c5b1e9ab8695afa3edce1b5d9</td>\n",
       "      <td>4</td>\n",
       "      <td>I generally liked the book. It had some parts that I felt were obvious but that is to be expected. The book also took many different turns that left me wanting to keep reading. The vampires are no...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2389899</th>\n",
       "      <td>7137327</td>\n",
       "      <td>9e332909706db667f28cd16567bbea07</td>\n",
       "      <td>3</td>\n",
       "      <td>To be honest I would probably rank it some where around 3.75. \\n I'll write a proper review later.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2388654 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          book_id                         review_id  rating  \\\n",
       "0         2767052  248c011811e945eca861b5c31a549291       5   \n",
       "1        23302416  84c0936a0f9868f38e75d2f9a5cb761e       5   \n",
       "2        18053080  785c8db878f4009da9741dea51f641da       4   \n",
       "3        17383543  34dc3c45d07e82718b05e73167259aef       2   \n",
       "4        16651458  d8d6b590780256fef7ae4a9550fe3e0d       5   \n",
       "...           ...                               ...     ...   \n",
       "2389895   6137154  1f6fc20b8187e8f36e12daf947b13e66       5   \n",
       "2389896  10193062  fc86ebe129a7f14a09c0221c92f52f79       3   \n",
       "2389897   6186357  15d0f64aa991f270c490e1a4d5b4011b       2   \n",
       "2389898  14740456  b048505c5b1e9ab8695afa3edce1b5d9       4   \n",
       "2389899   7137327  9e332909706db667f28cd16567bbea07       3   \n",
       "\n",
       "                                                                                                                                                                                                     review_text  \\\n",
       "0        I cracked and finally picked this up. Very enjoyable quick read - couldn't put it down - it was like crack. \\n I'm a bit bothered by the lack of backstory of how Panem and the Hunger Games come ab...   \n",
       "1        I read this book because my fifth grade son was required to for school. I'm so glad I did! I experienced a range of emotions & just loved it. Glad these middle schoolers are being exposed to the t...   \n",
       "2                                                                                         Though the book started out slow and only started to get interesting towards page 100, overall, it was worth the read.   \n",
       "3        *Update - 10/27/13* - After some sleep, I thinking about Allegiant overall without the influence of other reviews, I changed my rating to a 2 which is deserved. \\n I know that no author is going t...   \n",
       "4        This is a moving, heartbreaking, view into a life of an obese 16 year old boy in high school. Don't let the name \"butter\" cause you to overlook reading this book. You see the life of butter from h...   \n",
       "...                                                                                                                                                                                                          ...   \n",
       "2389895                         A really amazing book! It goes great with Graceling. It's connected to it in a way you don't realize until the end of either books. It really is amazing and semi-mind twisting.   \n",
       "2389896  This book is better then the first. It has it's slow points \\n (mostly the romance sections. I don't know why but it just doesn't interest me as much as it did in other books. Might be because I l...   \n",
       "2389897  I think this may have come from having expectations set to high. Everything I heard and read I was expecting to be blown away and I simply wasn't. I really don't like Thomas, I think he's an Idiot...   \n",
       "2389898  I generally liked the book. It had some parts that I felt were obvious but that is to be expected. The book also took many different turns that left me wanting to keep reading. The vampires are no...   \n",
       "2389899                                                                                                       To be honest I would probably rank it some where around 3.75. \\n I'll write a proper review later.   \n",
       "\n",
       "         n_votes  n_comments  \n",
       "0             24          25  \n",
       "1              0           0  \n",
       "2              0           0  \n",
       "3              0           0  \n",
       "4              0           0  \n",
       "...          ...         ...  \n",
       "2389895        0           0  \n",
       "2389896        0           0  \n",
       "2389897        0           0  \n",
       "2389898        0           0  \n",
       "2389899        0           0  \n",
       "\n",
       "[2388654 rows x 6 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ya_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For most part content in * * is not particularly valuable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if a review has one or more words in all caps, add a column to the dataframe with a 1, else 0\n",
    "ya_reviews['all_caps'] = ya_reviews['review_text'].str.contains(r'\\b[A-Z]{2,}\\b').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercase all text in review_text_clean\n",
    "ya_reviews['review_text_clean'] = ya_reviews['review_text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While some quotes contain actual review content, particularly sarcasm, more often than not it is actual book quotes. Removing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove text in * _ * format in review_text to a new column\n",
    "ya_reviews['review_text_clean'] = ya_reviews['review_text_clean'].str.replace('\\*.*?\\*', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop any rows containing \"see full review\" or http\n",
    "ya_reviews = ya_reviews[~ya_reviews['review_text_clean'].str.contains('see full review|http')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jesse\\AppData\\Local\\Temp\\ipykernel_29984\\2139168261.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  ya_reviews['review_text_clean'] = ya_reviews['review_text_clean'].apply(lambda x: contractions.fix(x))\n"
     ]
    }
   ],
   "source": [
    "import contractions\n",
    "#apply contractions fix to review_text_clean on the text level\n",
    "ya_reviews['review_text_clean'] = ya_reviews['review_text_clean'].apply(lambda x: contractions.fix(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ya_reviews['review_text_clean'] = ya_reviews['review_text_clean'].replace(r'\"[^\"]*\"', '', regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopword removal (NOT DOING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = {\n",
    "    ',': '',\n",
    "    ' - ': ' ',\n",
    "    ' -': ' ',\n",
    "    '- ': ' ',\n",
    "    '-': '',\n",
    "    '\\\\(': '',\n",
    "    '\\\\)': '',\n",
    "    '\\\\:': '',\n",
    "    '\\\\;': '',\n",
    "    '\\\\[': '',\n",
    "    '\\\\]': '',\n",
    "    '\\\\{': '',\n",
    "    '\\\\}': '',\n",
    "    \"\\\\'\": '',\n",
    "    \"\\\\.\": '',\n",
    "    '\\\\&': 'and',\n",
    "    '\\\\\"': ''\n",
    "}\n",
    "ya_reviews['review_text_clean'] = ya_reviews['review_text_clean'].replace(replacements, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove new lines from review_text_clean\n",
    "ya_reviews['review_text_clean'] = ya_reviews['review_text_clean'].str.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "noticing that some things are in weird combos of \" ' and ' \" so not getting properly removed UGH but gonna leave as is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ya reviews to pickles\n",
    "ya_reviews.to_pickle(DIR + 'ya_reviews.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
